{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install -q transformers datasets","metadata":{"id":"NCtvqtb1YKsp","execution":{"iopub.status.busy":"2022-03-26T17:29:14.402269Z","iopub.execute_input":"2022-03-26T17:29:14.403068Z","iopub.status.idle":"2022-03-26T17:29:23.417807Z","shell.execute_reply.started":"2022-03-26T17:29:14.402953Z","shell.execute_reply":"2022-03-26T17:29:23.416858Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n","output_type":"stream"}]},{"cell_type":"code","source":"from collections import Counter\n\nfrom datasets import load_dataset\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import precision_recall_fscore_support, accuracy_score\n\nimport os \n\nimport torch\nfrom torch import nn, optim\nimport transformers\nfrom transformers import BertTokenizer, BertModel, BertForSequenceClassification\nfrom transformers import Trainer, TrainingArguments","metadata":{"id":"OOyQri_3YtWe","execution":{"iopub.status.busy":"2022-03-26T17:29:23.419884Z","iopub.execute_input":"2022-03-26T17:29:23.420157Z","iopub.status.idle":"2022-03-26T17:29:30.039744Z","shell.execute_reply.started":"2022-03-26T17:29:23.420123Z","shell.execute_reply":"2022-03-26T17:29:30.038987Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"Download dataset","metadata":{"id":"cYaLmxqkZzof"}},{"cell_type":"code","source":"imdb_dataset = load_dataset('imdb')","metadata":{"id":"smO9l45uY0xr","outputId":"cf3f3670-7bdb-490a-bf99-ffcee6ef4752","execution":{"iopub.status.busy":"2022-03-26T17:29:30.041109Z","iopub.execute_input":"2022-03-26T17:29:30.041375Z","iopub.status.idle":"2022-03-26T17:30:13.015019Z","shell.execute_reply.started":"2022-03-26T17:29:30.041342Z","shell.execute_reply":"2022-03-26T17:30:13.014293Z"},"trusted":true},"execution_count":3,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/1.79k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ab7a3a81fb69418599ce2180a763620a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/1.05k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dd7b6168951747f78eb39adf6d1cdc10"}},"metadata":{}},{"name":"stdout","text":"Downloading and preparing dataset imdb/plain_text (download: 80.23 MiB, generated: 127.02 MiB, post-processed: Unknown size, total: 207.25 MiB) to /root/.cache/huggingface/datasets/imdb/plain_text/1.0.0/2fdd8b9bcadd6e7055e742a706876ba43f19faee861df134affd7a3f60fc38a1...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/84.1M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e34dcf5d81fe49cd940e368e7b87ae5d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Dataset imdb downloaded and prepared to /root/.cache/huggingface/datasets/imdb/plain_text/1.0.0/2fdd8b9bcadd6e7055e742a706876ba43f19faee861df134affd7a3f60fc38a1. Subsequent calls will reuse this data.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7f3104eb46bb48b59137d347eb8a2c11"}},"metadata":{}}]},{"cell_type":"code","source":"shuf_dataset = imdb_dataset.shuffle(seed=42)  #  shuffle so that there are different labels in our subset","metadata":{"id":"OYMPMRRIhnYc","outputId":"fab6695d-74c8-458a-918a-8fdacbc3860b","execution":{"iopub.status.busy":"2022-03-26T17:30:13.017060Z","iopub.execute_input":"2022-03-26T17:30:13.019661Z","iopub.status.idle":"2022-03-26T17:30:13.070959Z","shell.execute_reply.started":"2022-03-26T17:30:13.019621Z","shell.execute_reply":"2022-03-26T17:30:13.070219Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"train_texts = shuf_dataset[\"train\"][\"text\"][:5000]\ntrain_labels = shuf_dataset[\"train\"][\"label\"][:5000]\ntest_texts = shuf_dataset[\"test\"][\"text\"][:1500]\ntest_labels = shuf_dataset[\"test\"][\"label\"][:1500]\ntrain_texts, val_texts, train_labels, val_labels = train_test_split(train_texts, train_labels, test_size=.2)","metadata":{"id":"oX0GJJFuY1D0","execution":{"iopub.status.busy":"2022-03-26T17:30:13.072331Z","iopub.execute_input":"2022-03-26T17:30:13.072743Z","iopub.status.idle":"2022-03-26T17:30:13.938038Z","shell.execute_reply.started":"2022-03-26T17:30:13.072704Z","shell.execute_reply":"2022-03-26T17:30:13.937275Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"Counter(test_labels), Counter(train_labels)","metadata":{"id":"rdlopcb4icYK","outputId":"e9726cd4-3747-4959-e089-7611a96f6d3a","execution":{"iopub.status.busy":"2022-03-26T17:30:13.939400Z","iopub.execute_input":"2022-03-26T17:30:13.939648Z","iopub.status.idle":"2022-03-26T17:30:13.947142Z","shell.execute_reply.started":"2022-03-26T17:30:13.939616Z","shell.execute_reply":"2022-03-26T17:30:13.946358Z"},"trusted":true},"execution_count":6,"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"(Counter({1: 738, 0: 762}), Counter({0: 1996, 1: 2004}))"},"metadata":{}}]},{"cell_type":"code","source":"del imdb_dataset, shuf_dataset","metadata":{"id":"jAIpju5ZZo5w","execution":{"iopub.status.busy":"2022-03-26T17:30:13.948517Z","iopub.execute_input":"2022-03-26T17:30:13.949071Z","iopub.status.idle":"2022-03-26T17:30:13.957923Z","shell.execute_reply.started":"2022-03-26T17:30:13.949030Z","shell.execute_reply":"2022-03-26T17:30:13.957128Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"Prepare data","metadata":{"id":"yB236Qb2egQy"}},{"cell_type":"code","source":"PRE_TRAINED_MODEL_NAME = 'bert-base-uncased'\ntokenizer = BertTokenizer.from_pretrained(PRE_TRAINED_MODEL_NAME)","metadata":{"id":"xv_0137KZvGf","outputId":"d099befa-96a6-4306-f28b-3424070adedd","execution":{"iopub.status.busy":"2022-03-26T17:30:13.959687Z","iopub.execute_input":"2022-03-26T17:30:13.960109Z","iopub.status.idle":"2022-03-26T17:30:27.062626Z","shell.execute_reply.started":"2022-03-26T17:30:13.960070Z","shell.execute_reply":"2022-03-26T17:30:27.061646Z"},"trusted":true},"execution_count":8,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/28.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e67bc7b80d734532a4ca97ee48b1c244"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/226k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"15952ddca7364069b6c26994e1767342"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/455k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ec7009faeeb7490b9303a8f068596965"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/570 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"28459d008f55487dae84e6bdc64eef5d"}},"metadata":{}}]},{"cell_type":"code","source":"train_encodings = tokenizer(train_texts, truncation=True, padding=True)\nval_encodings = tokenizer(val_texts, truncation=True, padding=True)\ntest_encodings = tokenizer(test_texts, truncation=True, padding=True)","metadata":{"id":"Lap_mCbzdY57","execution":{"iopub.status.busy":"2022-03-26T17:30:27.064261Z","iopub.execute_input":"2022-03-26T17:30:27.064834Z","iopub.status.idle":"2022-03-26T17:31:18.423727Z","shell.execute_reply.started":"2022-03-26T17:30:27.064792Z","shell.execute_reply":"2022-03-26T17:31:18.422896Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"class IMDbDataset(torch.utils.data.Dataset):\n    def __init__(self, encodings, labels):\n        self.encodings = encodings\n        self.labels = labels\n\n    def __getitem__(self, idx):\n        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n        item['labels'] = torch.tensor(self.labels[idx])\n        return item\n\n    def __len__(self):\n        return len(self.labels)\n\ntrain_dataset = IMDbDataset(train_encodings, train_labels)\nval_dataset = IMDbDataset(val_encodings, val_labels)\ntest_dataset = IMDbDataset(test_encodings, test_labels)","metadata":{"id":"r4SMIz9ZeTi_","execution":{"iopub.status.busy":"2022-03-26T17:31:18.427258Z","iopub.execute_input":"2022-03-26T17:31:18.427514Z","iopub.status.idle":"2022-03-26T17:31:18.434082Z","shell.execute_reply.started":"2022-03-26T17:31:18.427481Z","shell.execute_reply":"2022-03-26T17:31:18.433326Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"Model","metadata":{"id":"qy4Utl1ybQPy"}},{"cell_type":"code","source":"device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")","metadata":{"id":"HihUNYOmdlHD","execution":{"iopub.status.busy":"2022-03-26T17:31:18.435674Z","iopub.execute_input":"2022-03-26T17:31:18.436008Z","iopub.status.idle":"2022-03-26T17:31:18.481583Z","shell.execute_reply.started":"2022-03-26T17:31:18.435964Z","shell.execute_reply":"2022-03-26T17:31:18.480648Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"A joint class model with bert output type as a parameter (tasks 1, 2 and 4) ","metadata":{}},{"cell_type":"code","source":"class SentimentClassifier(nn.Module):\n\n  def __init__(self, n_classes, output_type='pooled'):\n    super().__init__()\n    self.bert = BertModel.from_pretrained(PRE_TRAINED_MODEL_NAME)\n    self.drop = nn.Dropout(p=0.3)\n    self.n_classes = n_classes\n    self.output_type = output_type\n\n    if self.output_type in ['pooled', 'agg_cls']:\n      lin_input_size = self.bert.config.hidden_size\n    elif output_type=='add_cls':\n      lin_input_size = self.bert.config.hidden_size*2\n\n    self.out = nn.Linear(lin_input_size, n_classes)\n    self.loss = nn.CrossEntropyLoss()\n    \n  \n  def forward(self, input_ids, attention_mask, token_type_ids, labels):\n    last_hidden_state, pooled_output, hidden_states = self.bert(\n      input_ids=input_ids,\n      attention_mask=attention_mask,\n      return_dict=False, \n      output_hidden_states=True)\n    \n    if self.output_type=='pooled':  # only pooled output\n      bert_output = pooled_output\n    \n    elif self.output_type=='add_cls':  # pooled + cls from the last layer\n      bert_output = torch.cat((pooled_output, last_hidden_state[:, 0, :]), dim=1)\n    \n    elif self.output_type=='agg_cls':  # cls-token aggregated from 4 last layers\n      cls_embs = torch.cat(tuple([hidden_states[i][:, 0, :].unsqueeze(1) for i in [-3, -2, -1, 0]]), dim=1)\n      bert_output = torch.mean(cls_embs, dim=1)\n      \n    output = self.drop(bert_output)\n    logits = self.out(output)\n    loss = self.loss(logits.view(-1, self.n_classes), labels.view(-1))\n    return (loss, logits)","metadata":{"id":"NyU3_gs9cFiC","execution":{"iopub.status.busy":"2022-03-26T17:31:18.483298Z","iopub.execute_input":"2022-03-26T17:31:18.484088Z","iopub.status.idle":"2022-03-26T17:31:18.498216Z","shell.execute_reply.started":"2022-03-26T17:31:18.484040Z","shell.execute_reply":"2022-03-26T17:31:18.497560Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"# torch.cuda.empty_cache()\n# del model","metadata":{"id":"pBepuxo-0o3V","execution":{"iopub.status.busy":"2022-03-26T17:31:18.500123Z","iopub.execute_input":"2022-03-26T17:31:18.500527Z","iopub.status.idle":"2022-03-26T17:31:18.509533Z","shell.execute_reply.started":"2022-03-26T17:31:18.500488Z","shell.execute_reply":"2022-03-26T17:31:18.508800Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":"A basic model, task 1","metadata":{"id":"01krZQbWfSP5"}},{"cell_type":"code","source":"model = SentimentClassifier(n_classes=2, output_type='pooled')\nmodel = model.to(device)","metadata":{"id":"q6lK3vy-js3E","outputId":"246b2d56-9f22-498b-9701-a20b9332a59d","execution":{"iopub.status.busy":"2022-03-26T17:31:18.511175Z","iopub.execute_input":"2022-03-26T17:31:18.511507Z","iopub.status.idle":"2022-03-26T17:31:56.469631Z","shell.execute_reply.started":"2022-03-26T17:31:18.511429Z","shell.execute_reply":"2022-03-26T17:31:56.468880Z"},"trusted":true},"execution_count":14,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/420M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4cb96cdea8164313adad92a98bd90141"}},"metadata":{}},{"name":"stderr","text":"Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight']\n- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","output_type":"stream"}]},{"cell_type":"code","source":"os.makedirs(\"./results\", exist_ok=True)\nos.makedirs(\"./logs\", exist_ok=True)  \n\ntraining_args = TrainingArguments(\n    output_dir='./results',         \n    num_train_epochs=2,             \n    per_device_train_batch_size=8,  \n    per_device_eval_batch_size=16,   \n    warmup_steps=500,                \n    weight_decay=0.01,               \n    logging_dir='./logs',            \n    logging_steps=10,\n    report_to=None\n)","metadata":{"id":"5jl9W2W8fYpp","outputId":"26f1db4c-48c5-417a-96f7-5a96f6ed1fcb","execution":{"iopub.status.busy":"2022-03-26T17:31:56.470997Z","iopub.execute_input":"2022-03-26T17:31:56.471233Z","iopub.status.idle":"2022-03-26T17:31:56.479566Z","shell.execute_reply.started":"2022-03-26T17:31:56.471200Z","shell.execute_reply":"2022-03-26T17:31:56.478770Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"def compute_metrics(pred):\n    print(pred)\n    labels = pred.label_ids\n    preds = pred.predictions.argmax(-1)\n    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='binary')\n    acc = accuracy_score(labels, preds)\n    return {\n        'accuracy': acc,\n        'f1': f1,\n        'precision': precision,\n        'recall': recall\n    }","metadata":{"id":"hWFgPElUglgV","execution":{"iopub.status.busy":"2022-03-26T17:31:56.481058Z","iopub.execute_input":"2022-03-26T17:31:56.481381Z","iopub.status.idle":"2022-03-26T17:31:56.489955Z","shell.execute_reply.started":"2022-03-26T17:31:56.481296Z","shell.execute_reply":"2022-03-26T17:31:56.489213Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":"Train and evaluate task 1 model","metadata":{}},{"cell_type":"code","source":"#  disable wandb in kaggle\nimport wandb\nwandb.init(mode=\"disabled\")","metadata":{"execution":{"iopub.status.busy":"2022-03-26T17:31:56.491144Z","iopub.execute_input":"2022-03-26T17:31:56.491487Z","iopub.status.idle":"2022-03-26T17:31:58.199916Z","shell.execute_reply.started":"2022-03-26T17:31:56.491449Z","shell.execute_reply":"2022-03-26T17:31:58.199144Z"},"trusted":true},"execution_count":17,"outputs":[{"execution_count":17,"output_type":"execute_result","data":{"text/plain":""},"metadata":{}}]},{"cell_type":"code","source":"trainer = Trainer(\n    model=model,                         \n    args=training_args,                  \n    train_dataset=train_dataset,         \n    eval_dataset=val_dataset,            \n    compute_metrics = compute_metrics\n)\n\ntrainer.train()","metadata":{"id":"vg0fG6LFhHeo","outputId":"328b1021-c12a-43f2-82ff-fd8edde50a23","execution":{"iopub.status.busy":"2022-03-26T17:31:58.201639Z","iopub.execute_input":"2022-03-26T17:31:58.201900Z","iopub.status.idle":"2022-03-26T17:39:46.693198Z","shell.execute_reply.started":"2022-03-26T17:31:58.201865Z","shell.execute_reply":"2022-03-26T17:39:46.692529Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  FutureWarning,\n***** Running training *****\n  Num examples = 4000\n  Num Epochs = 2\n  Instantaneous batch size per device = 8\n  Total train batch size (w. parallel, distributed & accumulation) = 8\n  Gradient Accumulation steps = 1\n  Total optimization steps = 1000\nAutomatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='1000' max='1000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1000/1000 07:47, Epoch 2/2]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>10</td>\n      <td>0.685100</td>\n    </tr>\n    <tr>\n      <td>20</td>\n      <td>0.717200</td>\n    </tr>\n    <tr>\n      <td>30</td>\n      <td>0.678500</td>\n    </tr>\n    <tr>\n      <td>40</td>\n      <td>0.688700</td>\n    </tr>\n    <tr>\n      <td>50</td>\n      <td>0.672800</td>\n    </tr>\n    <tr>\n      <td>60</td>\n      <td>0.681200</td>\n    </tr>\n    <tr>\n      <td>70</td>\n      <td>0.675800</td>\n    </tr>\n    <tr>\n      <td>80</td>\n      <td>0.644400</td>\n    </tr>\n    <tr>\n      <td>90</td>\n      <td>0.632500</td>\n    </tr>\n    <tr>\n      <td>100</td>\n      <td>0.612500</td>\n    </tr>\n    <tr>\n      <td>110</td>\n      <td>0.546300</td>\n    </tr>\n    <tr>\n      <td>120</td>\n      <td>0.519200</td>\n    </tr>\n    <tr>\n      <td>130</td>\n      <td>0.502400</td>\n    </tr>\n    <tr>\n      <td>140</td>\n      <td>0.373700</td>\n    </tr>\n    <tr>\n      <td>150</td>\n      <td>0.404500</td>\n    </tr>\n    <tr>\n      <td>160</td>\n      <td>0.175800</td>\n    </tr>\n    <tr>\n      <td>170</td>\n      <td>0.419100</td>\n    </tr>\n    <tr>\n      <td>180</td>\n      <td>0.432800</td>\n    </tr>\n    <tr>\n      <td>190</td>\n      <td>0.417400</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>0.319000</td>\n    </tr>\n    <tr>\n      <td>210</td>\n      <td>0.369900</td>\n    </tr>\n    <tr>\n      <td>220</td>\n      <td>0.417800</td>\n    </tr>\n    <tr>\n      <td>230</td>\n      <td>0.597400</td>\n    </tr>\n    <tr>\n      <td>240</td>\n      <td>0.303000</td>\n    </tr>\n    <tr>\n      <td>250</td>\n      <td>0.350200</td>\n    </tr>\n    <tr>\n      <td>260</td>\n      <td>0.461400</td>\n    </tr>\n    <tr>\n      <td>270</td>\n      <td>0.466200</td>\n    </tr>\n    <tr>\n      <td>280</td>\n      <td>0.200000</td>\n    </tr>\n    <tr>\n      <td>290</td>\n      <td>0.443700</td>\n    </tr>\n    <tr>\n      <td>300</td>\n      <td>0.343800</td>\n    </tr>\n    <tr>\n      <td>310</td>\n      <td>0.329700</td>\n    </tr>\n    <tr>\n      <td>320</td>\n      <td>0.166000</td>\n    </tr>\n    <tr>\n      <td>330</td>\n      <td>0.479100</td>\n    </tr>\n    <tr>\n      <td>340</td>\n      <td>0.377000</td>\n    </tr>\n    <tr>\n      <td>350</td>\n      <td>0.217900</td>\n    </tr>\n    <tr>\n      <td>360</td>\n      <td>0.250400</td>\n    </tr>\n    <tr>\n      <td>370</td>\n      <td>0.479000</td>\n    </tr>\n    <tr>\n      <td>380</td>\n      <td>0.421000</td>\n    </tr>\n    <tr>\n      <td>390</td>\n      <td>0.458800</td>\n    </tr>\n    <tr>\n      <td>400</td>\n      <td>0.342400</td>\n    </tr>\n    <tr>\n      <td>410</td>\n      <td>0.194500</td>\n    </tr>\n    <tr>\n      <td>420</td>\n      <td>0.548500</td>\n    </tr>\n    <tr>\n      <td>430</td>\n      <td>0.218400</td>\n    </tr>\n    <tr>\n      <td>440</td>\n      <td>0.379700</td>\n    </tr>\n    <tr>\n      <td>450</td>\n      <td>0.418900</td>\n    </tr>\n    <tr>\n      <td>460</td>\n      <td>0.614000</td>\n    </tr>\n    <tr>\n      <td>470</td>\n      <td>0.343000</td>\n    </tr>\n    <tr>\n      <td>480</td>\n      <td>0.356400</td>\n    </tr>\n    <tr>\n      <td>490</td>\n      <td>0.954000</td>\n    </tr>\n    <tr>\n      <td>500</td>\n      <td>0.268500</td>\n    </tr>\n    <tr>\n      <td>510</td>\n      <td>0.309100</td>\n    </tr>\n    <tr>\n      <td>520</td>\n      <td>0.180700</td>\n    </tr>\n    <tr>\n      <td>530</td>\n      <td>0.125700</td>\n    </tr>\n    <tr>\n      <td>540</td>\n      <td>0.503200</td>\n    </tr>\n    <tr>\n      <td>550</td>\n      <td>0.216400</td>\n    </tr>\n    <tr>\n      <td>560</td>\n      <td>0.171500</td>\n    </tr>\n    <tr>\n      <td>570</td>\n      <td>0.373500</td>\n    </tr>\n    <tr>\n      <td>580</td>\n      <td>0.231700</td>\n    </tr>\n    <tr>\n      <td>590</td>\n      <td>0.231500</td>\n    </tr>\n    <tr>\n      <td>600</td>\n      <td>0.391800</td>\n    </tr>\n    <tr>\n      <td>610</td>\n      <td>0.303600</td>\n    </tr>\n    <tr>\n      <td>620</td>\n      <td>0.228900</td>\n    </tr>\n    <tr>\n      <td>630</td>\n      <td>0.337800</td>\n    </tr>\n    <tr>\n      <td>640</td>\n      <td>0.373800</td>\n    </tr>\n    <tr>\n      <td>650</td>\n      <td>0.483400</td>\n    </tr>\n    <tr>\n      <td>660</td>\n      <td>0.236200</td>\n    </tr>\n    <tr>\n      <td>670</td>\n      <td>0.188800</td>\n    </tr>\n    <tr>\n      <td>680</td>\n      <td>0.262600</td>\n    </tr>\n    <tr>\n      <td>690</td>\n      <td>0.377400</td>\n    </tr>\n    <tr>\n      <td>700</td>\n      <td>0.101200</td>\n    </tr>\n    <tr>\n      <td>710</td>\n      <td>0.259000</td>\n    </tr>\n    <tr>\n      <td>720</td>\n      <td>0.204800</td>\n    </tr>\n    <tr>\n      <td>730</td>\n      <td>0.500700</td>\n    </tr>\n    <tr>\n      <td>740</td>\n      <td>0.334300</td>\n    </tr>\n    <tr>\n      <td>750</td>\n      <td>0.353200</td>\n    </tr>\n    <tr>\n      <td>760</td>\n      <td>0.129700</td>\n    </tr>\n    <tr>\n      <td>770</td>\n      <td>0.259100</td>\n    </tr>\n    <tr>\n      <td>780</td>\n      <td>0.689700</td>\n    </tr>\n    <tr>\n      <td>790</td>\n      <td>0.086100</td>\n    </tr>\n    <tr>\n      <td>800</td>\n      <td>0.332900</td>\n    </tr>\n    <tr>\n      <td>810</td>\n      <td>0.118500</td>\n    </tr>\n    <tr>\n      <td>820</td>\n      <td>0.325800</td>\n    </tr>\n    <tr>\n      <td>830</td>\n      <td>0.132400</td>\n    </tr>\n    <tr>\n      <td>840</td>\n      <td>0.349300</td>\n    </tr>\n    <tr>\n      <td>850</td>\n      <td>0.150400</td>\n    </tr>\n    <tr>\n      <td>860</td>\n      <td>0.256600</td>\n    </tr>\n    <tr>\n      <td>870</td>\n      <td>0.110500</td>\n    </tr>\n    <tr>\n      <td>880</td>\n      <td>0.318300</td>\n    </tr>\n    <tr>\n      <td>890</td>\n      <td>0.258700</td>\n    </tr>\n    <tr>\n      <td>900</td>\n      <td>0.188400</td>\n    </tr>\n    <tr>\n      <td>910</td>\n      <td>0.137200</td>\n    </tr>\n    <tr>\n      <td>920</td>\n      <td>0.242900</td>\n    </tr>\n    <tr>\n      <td>930</td>\n      <td>0.450200</td>\n    </tr>\n    <tr>\n      <td>940</td>\n      <td>0.339500</td>\n    </tr>\n    <tr>\n      <td>950</td>\n      <td>0.242600</td>\n    </tr>\n    <tr>\n      <td>960</td>\n      <td>0.101800</td>\n    </tr>\n    <tr>\n      <td>970</td>\n      <td>0.144600</td>\n    </tr>\n    <tr>\n      <td>980</td>\n      <td>0.378800</td>\n    </tr>\n    <tr>\n      <td>990</td>\n      <td>0.084300</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>0.153800</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"Saving model checkpoint to ./results/checkpoint-500\nTrainer.model is not a `PreTrainedModel`, only saving its state dict.\nSaving model checkpoint to ./results/checkpoint-1000\nTrainer.model is not a `PreTrainedModel`, only saving its state dict.\n\n\nTraining completed. Do not forget to share your model on huggingface.co/models =)\n\n\n","output_type":"stream"},{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=1000, training_loss=0.3583272613286972, metrics={'train_runtime': 468.3797, 'train_samples_per_second': 17.08, 'train_steps_per_second': 2.135, 'total_flos': 0.0, 'train_loss': 0.3583272613286972, 'epoch': 2.0})"},"metadata":{}}]},{"cell_type":"code","source":"trainer.evaluate(eval_dataset=test_dataset, metric_key_prefix=\"test\")","metadata":{"id":"0syQTBgifnBF","execution":{"iopub.status.busy":"2022-03-26T17:39:46.696811Z","iopub.execute_input":"2022-03-26T17:39:46.699259Z","iopub.status.idle":"2022-03-26T17:40:14.088216Z","shell.execute_reply.started":"2022-03-26T17:39:46.699220Z","shell.execute_reply":"2022-03-26T17:40:14.087542Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stderr","text":"***** Running Evaluation *****\n  Num examples = 1500\n  Batch size = 16\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='94' max='94' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [94/94 00:27]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"EvalPrediction(predictions=array([[-3.4775002,  3.262797 ],\n       [ 2.1396692, -1.3572104],\n       [ 2.971143 , -2.2397625],\n       ...,\n       [-3.1341858,  3.1159618],\n       [ 2.0713255, -1.444596 ],\n       [-2.5953648,  2.612521 ]], dtype=float32), label_ids=array([1, 1, 0, ..., 1, 0, 1]))\n","output_type":"stream"},{"execution_count":19,"output_type":"execute_result","data":{"text/plain":"{'test_loss': 0.3452337384223938,\n 'test_accuracy': 0.9093333333333333,\n 'test_f1': 0.9092122830440588,\n 'test_precision': 0.8960526315789473,\n 'test_recall': 0.9227642276422764,\n 'test_runtime': 27.3771,\n 'test_samples_per_second': 54.79,\n 'test_steps_per_second': 3.434,\n 'epoch': 2.0}"},"metadata":{}}]},{"cell_type":"markdown","source":"Add CLS-token embedding","metadata":{"id":"JOL06qIDXpbu"}},{"cell_type":"code","source":"model_with_cls = SentimentClassifier(n_classes=2, output_type='add_cls')\nmodel_with_cls = model_with_cls.to(device)","metadata":{"id":"B68MWo6e_TnA","outputId":"d1a4d565-4157-4d7d-8c3c-0ce620287729","execution":{"iopub.status.busy":"2022-03-26T17:40:14.089594Z","iopub.execute_input":"2022-03-26T17:40:14.089843Z","iopub.status.idle":"2022-03-26T17:40:17.666175Z","shell.execute_reply.started":"2022-03-26T17:40:14.089809Z","shell.execute_reply":"2022-03-26T17:40:17.665370Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stderr","text":"loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\nModel config BertConfig {\n  \"architectures\": [\n    \"BertForMaskedLM\"\n  ],\n  \"attention_probs_dropout_prob\": 0.1,\n  \"classifier_dropout\": null,\n  \"gradient_checkpointing\": false,\n  \"hidden_act\": \"gelu\",\n  \"hidden_dropout_prob\": 0.1,\n  \"hidden_size\": 768,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 3072,\n  \"layer_norm_eps\": 1e-12,\n  \"max_position_embeddings\": 512,\n  \"model_type\": \"bert\",\n  \"num_attention_heads\": 12,\n  \"num_hidden_layers\": 12,\n  \"pad_token_id\": 0,\n  \"position_embedding_type\": \"absolute\",\n  \"transformers_version\": \"4.16.2\",\n  \"type_vocab_size\": 2,\n  \"use_cache\": true,\n  \"vocab_size\": 30522\n}\n\nloading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f\nSome weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight']\n- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nAll the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.\nIf your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.\n","output_type":"stream"}]},{"cell_type":"code","source":"trainer = Trainer(\n    model=model_with_cls,                         \n    args=training_args,                  \n    train_dataset=train_dataset,         \n    eval_dataset=val_dataset,            \n    compute_metrics = compute_metrics    \n)\n\ntrainer.train()","metadata":{"id":"n85W6JnyX_8d","outputId":"d7a2aee0-34c3-4c58-fc14-3c06747f135b","execution":{"iopub.status.busy":"2022-03-26T17:40:17.667640Z","iopub.execute_input":"2022-03-26T17:40:17.667918Z","iopub.status.idle":"2022-03-26T17:48:07.268108Z","shell.execute_reply.started":"2022-03-26T17:40:17.667881Z","shell.execute_reply":"2022-03-26T17:48:07.267328Z"},"trusted":true},"execution_count":21,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  FutureWarning,\n***** Running training *****\n  Num examples = 4000\n  Num Epochs = 2\n  Instantaneous batch size per device = 8\n  Total train batch size (w. parallel, distributed & accumulation) = 8\n  Gradient Accumulation steps = 1\n  Total optimization steps = 1000\nAutomatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='1000' max='1000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1000/1000 07:49, Epoch 2/2]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>10</td>\n      <td>0.680200</td>\n    </tr>\n    <tr>\n      <td>20</td>\n      <td>0.687800</td>\n    </tr>\n    <tr>\n      <td>30</td>\n      <td>0.710200</td>\n    </tr>\n    <tr>\n      <td>40</td>\n      <td>0.688300</td>\n    </tr>\n    <tr>\n      <td>50</td>\n      <td>0.677400</td>\n    </tr>\n    <tr>\n      <td>60</td>\n      <td>0.695600</td>\n    </tr>\n    <tr>\n      <td>70</td>\n      <td>0.655800</td>\n    </tr>\n    <tr>\n      <td>80</td>\n      <td>0.603600</td>\n    </tr>\n    <tr>\n      <td>90</td>\n      <td>0.585200</td>\n    </tr>\n    <tr>\n      <td>100</td>\n      <td>0.542200</td>\n    </tr>\n    <tr>\n      <td>110</td>\n      <td>0.506700</td>\n    </tr>\n    <tr>\n      <td>120</td>\n      <td>0.497900</td>\n    </tr>\n    <tr>\n      <td>130</td>\n      <td>0.550800</td>\n    </tr>\n    <tr>\n      <td>140</td>\n      <td>0.351800</td>\n    </tr>\n    <tr>\n      <td>150</td>\n      <td>0.340400</td>\n    </tr>\n    <tr>\n      <td>160</td>\n      <td>0.158900</td>\n    </tr>\n    <tr>\n      <td>170</td>\n      <td>0.347200</td>\n    </tr>\n    <tr>\n      <td>180</td>\n      <td>0.567000</td>\n    </tr>\n    <tr>\n      <td>190</td>\n      <td>0.488900</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>0.318800</td>\n    </tr>\n    <tr>\n      <td>210</td>\n      <td>0.351800</td>\n    </tr>\n    <tr>\n      <td>220</td>\n      <td>0.481100</td>\n    </tr>\n    <tr>\n      <td>230</td>\n      <td>0.554900</td>\n    </tr>\n    <tr>\n      <td>240</td>\n      <td>0.280900</td>\n    </tr>\n    <tr>\n      <td>250</td>\n      <td>0.339700</td>\n    </tr>\n    <tr>\n      <td>260</td>\n      <td>0.526900</td>\n    </tr>\n    <tr>\n      <td>270</td>\n      <td>0.319900</td>\n    </tr>\n    <tr>\n      <td>280</td>\n      <td>0.161100</td>\n    </tr>\n    <tr>\n      <td>290</td>\n      <td>0.571500</td>\n    </tr>\n    <tr>\n      <td>300</td>\n      <td>0.454900</td>\n    </tr>\n    <tr>\n      <td>310</td>\n      <td>0.242000</td>\n    </tr>\n    <tr>\n      <td>320</td>\n      <td>0.195800</td>\n    </tr>\n    <tr>\n      <td>330</td>\n      <td>0.812300</td>\n    </tr>\n    <tr>\n      <td>340</td>\n      <td>0.401000</td>\n    </tr>\n    <tr>\n      <td>350</td>\n      <td>0.311600</td>\n    </tr>\n    <tr>\n      <td>360</td>\n      <td>0.431100</td>\n    </tr>\n    <tr>\n      <td>370</td>\n      <td>0.646700</td>\n    </tr>\n    <tr>\n      <td>380</td>\n      <td>0.371300</td>\n    </tr>\n    <tr>\n      <td>390</td>\n      <td>0.384400</td>\n    </tr>\n    <tr>\n      <td>400</td>\n      <td>0.245700</td>\n    </tr>\n    <tr>\n      <td>410</td>\n      <td>0.187900</td>\n    </tr>\n    <tr>\n      <td>420</td>\n      <td>0.390000</td>\n    </tr>\n    <tr>\n      <td>430</td>\n      <td>0.435500</td>\n    </tr>\n    <tr>\n      <td>440</td>\n      <td>0.417500</td>\n    </tr>\n    <tr>\n      <td>450</td>\n      <td>0.448200</td>\n    </tr>\n    <tr>\n      <td>460</td>\n      <td>0.323800</td>\n    </tr>\n    <tr>\n      <td>470</td>\n      <td>0.341800</td>\n    </tr>\n    <tr>\n      <td>480</td>\n      <td>0.301300</td>\n    </tr>\n    <tr>\n      <td>490</td>\n      <td>0.722600</td>\n    </tr>\n    <tr>\n      <td>500</td>\n      <td>0.331300</td>\n    </tr>\n    <tr>\n      <td>510</td>\n      <td>0.251800</td>\n    </tr>\n    <tr>\n      <td>520</td>\n      <td>0.188000</td>\n    </tr>\n    <tr>\n      <td>530</td>\n      <td>0.133200</td>\n    </tr>\n    <tr>\n      <td>540</td>\n      <td>0.627500</td>\n    </tr>\n    <tr>\n      <td>550</td>\n      <td>0.335300</td>\n    </tr>\n    <tr>\n      <td>560</td>\n      <td>0.261800</td>\n    </tr>\n    <tr>\n      <td>570</td>\n      <td>0.244300</td>\n    </tr>\n    <tr>\n      <td>580</td>\n      <td>0.373800</td>\n    </tr>\n    <tr>\n      <td>590</td>\n      <td>0.093700</td>\n    </tr>\n    <tr>\n      <td>600</td>\n      <td>0.224500</td>\n    </tr>\n    <tr>\n      <td>610</td>\n      <td>0.233400</td>\n    </tr>\n    <tr>\n      <td>620</td>\n      <td>0.377200</td>\n    </tr>\n    <tr>\n      <td>630</td>\n      <td>0.290300</td>\n    </tr>\n    <tr>\n      <td>640</td>\n      <td>0.290800</td>\n    </tr>\n    <tr>\n      <td>650</td>\n      <td>0.419300</td>\n    </tr>\n    <tr>\n      <td>660</td>\n      <td>0.149800</td>\n    </tr>\n    <tr>\n      <td>670</td>\n      <td>0.140200</td>\n    </tr>\n    <tr>\n      <td>680</td>\n      <td>0.205500</td>\n    </tr>\n    <tr>\n      <td>690</td>\n      <td>0.163200</td>\n    </tr>\n    <tr>\n      <td>700</td>\n      <td>0.133800</td>\n    </tr>\n    <tr>\n      <td>710</td>\n      <td>0.165000</td>\n    </tr>\n    <tr>\n      <td>720</td>\n      <td>0.406800</td>\n    </tr>\n    <tr>\n      <td>730</td>\n      <td>0.394500</td>\n    </tr>\n    <tr>\n      <td>740</td>\n      <td>0.369200</td>\n    </tr>\n    <tr>\n      <td>750</td>\n      <td>0.224800</td>\n    </tr>\n    <tr>\n      <td>760</td>\n      <td>0.159400</td>\n    </tr>\n    <tr>\n      <td>770</td>\n      <td>0.123600</td>\n    </tr>\n    <tr>\n      <td>780</td>\n      <td>0.693200</td>\n    </tr>\n    <tr>\n      <td>790</td>\n      <td>0.040500</td>\n    </tr>\n    <tr>\n      <td>800</td>\n      <td>0.506100</td>\n    </tr>\n    <tr>\n      <td>810</td>\n      <td>0.134800</td>\n    </tr>\n    <tr>\n      <td>820</td>\n      <td>0.306400</td>\n    </tr>\n    <tr>\n      <td>830</td>\n      <td>0.196700</td>\n    </tr>\n    <tr>\n      <td>840</td>\n      <td>0.389700</td>\n    </tr>\n    <tr>\n      <td>850</td>\n      <td>0.155800</td>\n    </tr>\n    <tr>\n      <td>860</td>\n      <td>0.071300</td>\n    </tr>\n    <tr>\n      <td>870</td>\n      <td>0.270400</td>\n    </tr>\n    <tr>\n      <td>880</td>\n      <td>0.327100</td>\n    </tr>\n    <tr>\n      <td>890</td>\n      <td>0.254400</td>\n    </tr>\n    <tr>\n      <td>900</td>\n      <td>0.166800</td>\n    </tr>\n    <tr>\n      <td>910</td>\n      <td>0.083000</td>\n    </tr>\n    <tr>\n      <td>920</td>\n      <td>0.097300</td>\n    </tr>\n    <tr>\n      <td>930</td>\n      <td>0.518700</td>\n    </tr>\n    <tr>\n      <td>940</td>\n      <td>0.272700</td>\n    </tr>\n    <tr>\n      <td>950</td>\n      <td>0.421700</td>\n    </tr>\n    <tr>\n      <td>960</td>\n      <td>0.240100</td>\n    </tr>\n    <tr>\n      <td>970</td>\n      <td>0.166700</td>\n    </tr>\n    <tr>\n      <td>980</td>\n      <td>0.240700</td>\n    </tr>\n    <tr>\n      <td>990</td>\n      <td>0.175300</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>0.115200</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"Saving model checkpoint to ./results/checkpoint-500\nTrainer.model is not a `PreTrainedModel`, only saving its state dict.\nSaving model checkpoint to ./results/checkpoint-1000\nTrainer.model is not a `PreTrainedModel`, only saving its state dict.\n\n\nTraining completed. Do not forget to share your model on huggingface.co/models =)\n\n\n","output_type":"stream"},{"execution_count":21,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=1000, training_loss=0.3546455535292625, metrics={'train_runtime': 469.5688, 'train_samples_per_second': 17.037, 'train_steps_per_second': 2.13, 'total_flos': 0.0, 'train_loss': 0.3546455535292625, 'epoch': 2.0})"},"metadata":{}}]},{"cell_type":"code","source":"trainer.evaluate(eval_dataset=test_dataset, metric_key_prefix=\"test\")","metadata":{"id":"WLH22hiXf4sx","execution":{"iopub.status.busy":"2022-03-26T17:48:07.269500Z","iopub.execute_input":"2022-03-26T17:48:07.270337Z","iopub.status.idle":"2022-03-26T17:48:34.674142Z","shell.execute_reply.started":"2022-03-26T17:48:07.270281Z","shell.execute_reply":"2022-03-26T17:48:34.673450Z"},"trusted":true},"execution_count":22,"outputs":[{"name":"stderr","text":"***** Running Evaluation *****\n  Num examples = 1500\n  Batch size = 16\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='94' max='94' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [94/94 00:27]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"EvalPrediction(predictions=array([[-2.8728597 ,  3.4979503 ],\n       [-0.24992041,  0.20069085],\n       [ 2.685428  , -3.3175306 ],\n       ...,\n       [-2.7161362 ,  3.3027742 ],\n       [ 1.0207161 , -1.2998348 ],\n       [-2.2120135 ,  2.776309  ]], dtype=float32), label_ids=array([1, 1, 0, ..., 1, 0, 1]))\n","output_type":"stream"},{"execution_count":22,"output_type":"execute_result","data":{"text/plain":"{'test_loss': 0.3123458921909332,\n 'test_accuracy': 0.9133333333333333,\n 'test_f1': 0.9136786188579018,\n 'test_precision': 0.8958333333333334,\n 'test_recall': 0.9322493224932249,\n 'test_runtime': 27.3932,\n 'test_samples_per_second': 54.758,\n 'test_steps_per_second': 3.432,\n 'epoch': 2.0}"},"metadata":{}}]},{"cell_type":"markdown","source":"Task 4, aggregate CLS tokens","metadata":{}},{"cell_type":"code","source":"model_agg_cls = SentimentClassifier(n_classes=2, output_type='agg_cls')\nmodel_agg_cls = model_agg_cls.to(device)","metadata":{"execution":{"iopub.status.busy":"2022-03-26T18:06:31.281107Z","iopub.execute_input":"2022-03-26T18:06:31.281384Z","iopub.status.idle":"2022-03-26T18:06:34.733844Z","shell.execute_reply.started":"2022-03-26T18:06:31.281355Z","shell.execute_reply":"2022-03-26T18:06:34.733027Z"},"trusted":true},"execution_count":35,"outputs":[{"name":"stderr","text":"loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\nModel config BertConfig {\n  \"architectures\": [\n    \"BertForMaskedLM\"\n  ],\n  \"attention_probs_dropout_prob\": 0.1,\n  \"classifier_dropout\": null,\n  \"gradient_checkpointing\": false,\n  \"hidden_act\": \"gelu\",\n  \"hidden_dropout_prob\": 0.1,\n  \"hidden_size\": 768,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 3072,\n  \"layer_norm_eps\": 1e-12,\n  \"max_position_embeddings\": 512,\n  \"model_type\": \"bert\",\n  \"num_attention_heads\": 12,\n  \"num_hidden_layers\": 12,\n  \"pad_token_id\": 0,\n  \"position_embedding_type\": \"absolute\",\n  \"transformers_version\": \"4.16.2\",\n  \"type_vocab_size\": 2,\n  \"use_cache\": true,\n  \"vocab_size\": 30522\n}\n\nloading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f\nSome weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight']\n- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nAll the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.\nIf your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.\n","output_type":"stream"}]},{"cell_type":"code","source":"trainer = Trainer(\n    model=model_agg_cls,                         \n    args=training_args,                  \n    train_dataset=train_dataset,         \n    eval_dataset=val_dataset,            \n    compute_metrics = compute_metrics    \n)\n\ntrainer.train()","metadata":{"execution":{"iopub.status.busy":"2022-03-26T18:06:34.735579Z","iopub.execute_input":"2022-03-26T18:06:34.735824Z","iopub.status.idle":"2022-03-26T18:14:23.914028Z","shell.execute_reply.started":"2022-03-26T18:06:34.735792Z","shell.execute_reply":"2022-03-26T18:14:23.912931Z"},"trusted":true},"execution_count":36,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  FutureWarning,\n***** Running training *****\n  Num examples = 4000\n  Num Epochs = 2\n  Instantaneous batch size per device = 8\n  Total train batch size (w. parallel, distributed & accumulation) = 8\n  Gradient Accumulation steps = 1\n  Total optimization steps = 1000\nAutomatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='1000' max='1000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1000/1000 07:48, Epoch 2/2]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>10</td>\n      <td>0.671800</td>\n    </tr>\n    <tr>\n      <td>20</td>\n      <td>0.657300</td>\n    </tr>\n    <tr>\n      <td>30</td>\n      <td>0.651200</td>\n    </tr>\n    <tr>\n      <td>40</td>\n      <td>0.669600</td>\n    </tr>\n    <tr>\n      <td>50</td>\n      <td>0.679700</td>\n    </tr>\n    <tr>\n      <td>60</td>\n      <td>0.663600</td>\n    </tr>\n    <tr>\n      <td>70</td>\n      <td>0.666900</td>\n    </tr>\n    <tr>\n      <td>80</td>\n      <td>0.618600</td>\n    </tr>\n    <tr>\n      <td>90</td>\n      <td>0.613200</td>\n    </tr>\n    <tr>\n      <td>100</td>\n      <td>0.558000</td>\n    </tr>\n    <tr>\n      <td>110</td>\n      <td>0.593500</td>\n    </tr>\n    <tr>\n      <td>120</td>\n      <td>0.488400</td>\n    </tr>\n    <tr>\n      <td>130</td>\n      <td>0.450300</td>\n    </tr>\n    <tr>\n      <td>140</td>\n      <td>0.343600</td>\n    </tr>\n    <tr>\n      <td>150</td>\n      <td>0.378400</td>\n    </tr>\n    <tr>\n      <td>160</td>\n      <td>0.139300</td>\n    </tr>\n    <tr>\n      <td>170</td>\n      <td>0.310700</td>\n    </tr>\n    <tr>\n      <td>180</td>\n      <td>0.482200</td>\n    </tr>\n    <tr>\n      <td>190</td>\n      <td>0.386200</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>0.244500</td>\n    </tr>\n    <tr>\n      <td>210</td>\n      <td>0.438500</td>\n    </tr>\n    <tr>\n      <td>220</td>\n      <td>0.412900</td>\n    </tr>\n    <tr>\n      <td>230</td>\n      <td>0.536200</td>\n    </tr>\n    <tr>\n      <td>240</td>\n      <td>0.289500</td>\n    </tr>\n    <tr>\n      <td>250</td>\n      <td>0.342300</td>\n    </tr>\n    <tr>\n      <td>260</td>\n      <td>0.458200</td>\n    </tr>\n    <tr>\n      <td>270</td>\n      <td>0.378500</td>\n    </tr>\n    <tr>\n      <td>280</td>\n      <td>0.167100</td>\n    </tr>\n    <tr>\n      <td>290</td>\n      <td>0.442800</td>\n    </tr>\n    <tr>\n      <td>300</td>\n      <td>0.402700</td>\n    </tr>\n    <tr>\n      <td>310</td>\n      <td>0.402600</td>\n    </tr>\n    <tr>\n      <td>320</td>\n      <td>0.196600</td>\n    </tr>\n    <tr>\n      <td>330</td>\n      <td>0.544100</td>\n    </tr>\n    <tr>\n      <td>340</td>\n      <td>0.334700</td>\n    </tr>\n    <tr>\n      <td>350</td>\n      <td>0.376300</td>\n    </tr>\n    <tr>\n      <td>360</td>\n      <td>0.301100</td>\n    </tr>\n    <tr>\n      <td>370</td>\n      <td>0.575400</td>\n    </tr>\n    <tr>\n      <td>380</td>\n      <td>0.334300</td>\n    </tr>\n    <tr>\n      <td>390</td>\n      <td>0.501600</td>\n    </tr>\n    <tr>\n      <td>400</td>\n      <td>0.347400</td>\n    </tr>\n    <tr>\n      <td>410</td>\n      <td>0.233600</td>\n    </tr>\n    <tr>\n      <td>420</td>\n      <td>0.333900</td>\n    </tr>\n    <tr>\n      <td>430</td>\n      <td>0.268100</td>\n    </tr>\n    <tr>\n      <td>440</td>\n      <td>0.490200</td>\n    </tr>\n    <tr>\n      <td>450</td>\n      <td>0.508800</td>\n    </tr>\n    <tr>\n      <td>460</td>\n      <td>0.303500</td>\n    </tr>\n    <tr>\n      <td>470</td>\n      <td>0.309800</td>\n    </tr>\n    <tr>\n      <td>480</td>\n      <td>0.244800</td>\n    </tr>\n    <tr>\n      <td>490</td>\n      <td>0.780800</td>\n    </tr>\n    <tr>\n      <td>500</td>\n      <td>0.404500</td>\n    </tr>\n    <tr>\n      <td>510</td>\n      <td>0.341700</td>\n    </tr>\n    <tr>\n      <td>520</td>\n      <td>0.240500</td>\n    </tr>\n    <tr>\n      <td>530</td>\n      <td>0.077400</td>\n    </tr>\n    <tr>\n      <td>540</td>\n      <td>0.491500</td>\n    </tr>\n    <tr>\n      <td>550</td>\n      <td>0.185700</td>\n    </tr>\n    <tr>\n      <td>560</td>\n      <td>0.196300</td>\n    </tr>\n    <tr>\n      <td>570</td>\n      <td>0.320100</td>\n    </tr>\n    <tr>\n      <td>580</td>\n      <td>0.124200</td>\n    </tr>\n    <tr>\n      <td>590</td>\n      <td>0.221900</td>\n    </tr>\n    <tr>\n      <td>600</td>\n      <td>0.493600</td>\n    </tr>\n    <tr>\n      <td>610</td>\n      <td>0.272800</td>\n    </tr>\n    <tr>\n      <td>620</td>\n      <td>0.153200</td>\n    </tr>\n    <tr>\n      <td>630</td>\n      <td>0.197200</td>\n    </tr>\n    <tr>\n      <td>640</td>\n      <td>0.290300</td>\n    </tr>\n    <tr>\n      <td>650</td>\n      <td>0.383400</td>\n    </tr>\n    <tr>\n      <td>660</td>\n      <td>0.111800</td>\n    </tr>\n    <tr>\n      <td>670</td>\n      <td>0.231900</td>\n    </tr>\n    <tr>\n      <td>680</td>\n      <td>0.158000</td>\n    </tr>\n    <tr>\n      <td>690</td>\n      <td>0.488800</td>\n    </tr>\n    <tr>\n      <td>700</td>\n      <td>0.116200</td>\n    </tr>\n    <tr>\n      <td>710</td>\n      <td>0.187700</td>\n    </tr>\n    <tr>\n      <td>720</td>\n      <td>0.454100</td>\n    </tr>\n    <tr>\n      <td>730</td>\n      <td>0.313100</td>\n    </tr>\n    <tr>\n      <td>740</td>\n      <td>0.314300</td>\n    </tr>\n    <tr>\n      <td>750</td>\n      <td>0.206500</td>\n    </tr>\n    <tr>\n      <td>760</td>\n      <td>0.201000</td>\n    </tr>\n    <tr>\n      <td>770</td>\n      <td>0.173500</td>\n    </tr>\n    <tr>\n      <td>780</td>\n      <td>0.602100</td>\n    </tr>\n    <tr>\n      <td>790</td>\n      <td>0.021700</td>\n    </tr>\n    <tr>\n      <td>800</td>\n      <td>0.494600</td>\n    </tr>\n    <tr>\n      <td>810</td>\n      <td>0.093100</td>\n    </tr>\n    <tr>\n      <td>820</td>\n      <td>0.265600</td>\n    </tr>\n    <tr>\n      <td>830</td>\n      <td>0.129600</td>\n    </tr>\n    <tr>\n      <td>840</td>\n      <td>0.230800</td>\n    </tr>\n    <tr>\n      <td>850</td>\n      <td>0.166000</td>\n    </tr>\n    <tr>\n      <td>860</td>\n      <td>0.045500</td>\n    </tr>\n    <tr>\n      <td>870</td>\n      <td>0.249800</td>\n    </tr>\n    <tr>\n      <td>880</td>\n      <td>0.305500</td>\n    </tr>\n    <tr>\n      <td>890</td>\n      <td>0.235200</td>\n    </tr>\n    <tr>\n      <td>900</td>\n      <td>0.173000</td>\n    </tr>\n    <tr>\n      <td>910</td>\n      <td>0.204700</td>\n    </tr>\n    <tr>\n      <td>920</td>\n      <td>0.170100</td>\n    </tr>\n    <tr>\n      <td>930</td>\n      <td>0.262400</td>\n    </tr>\n    <tr>\n      <td>940</td>\n      <td>0.306000</td>\n    </tr>\n    <tr>\n      <td>950</td>\n      <td>0.249900</td>\n    </tr>\n    <tr>\n      <td>960</td>\n      <td>0.181800</td>\n    </tr>\n    <tr>\n      <td>970</td>\n      <td>0.173000</td>\n    </tr>\n    <tr>\n      <td>980</td>\n      <td>0.402400</td>\n    </tr>\n    <tr>\n      <td>990</td>\n      <td>0.098000</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>0.101900</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"Saving model checkpoint to ./results/checkpoint-500\nTrainer.model is not a `PreTrainedModel`, only saving its state dict.\nSaving model checkpoint to ./results/checkpoint-1000\nTrainer.model is not a `PreTrainedModel`, only saving its state dict.\n\n\nTraining completed. Do not forget to share your model on huggingface.co/models =)\n\n\n","output_type":"stream"},{"execution_count":36,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=1000, training_loss=0.3403690246641636, metrics={'train_runtime': 469.1292, 'train_samples_per_second': 17.053, 'train_steps_per_second': 2.132, 'total_flos': 0.0, 'train_loss': 0.3403690246641636, 'epoch': 2.0})"},"metadata":{}}]},{"cell_type":"code","source":"trainer.evaluate(eval_dataset=test_dataset, metric_key_prefix=\"test\")","metadata":{"execution":{"iopub.status.busy":"2022-03-26T18:14:23.918747Z","iopub.execute_input":"2022-03-26T18:14:23.921025Z","iopub.status.idle":"2022-03-26T18:14:51.353979Z","shell.execute_reply.started":"2022-03-26T18:14:23.920979Z","shell.execute_reply":"2022-03-26T18:14:51.353195Z"},"trusted":true},"execution_count":37,"outputs":[{"name":"stderr","text":"***** Running Evaluation *****\n  Num examples = 1500\n  Batch size = 16\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='94' max='94' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [94/94 00:27]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"EvalPrediction(predictions=array([[-3.6609862 ,  3.4471428 ],\n       [ 0.4066905 , -0.44584104],\n       [ 3.0857577 , -2.9067018 ],\n       ...,\n       [-3.1954618 ,  2.962535  ],\n       [ 1.1746558 , -1.2398993 ],\n       [-1.4246486 ,  1.3980545 ]], dtype=float32), label_ids=array([1, 1, 0, ..., 1, 0, 1]))\n","output_type":"stream"},{"execution_count":37,"output_type":"execute_result","data":{"text/plain":"{'test_loss': 0.3542517125606537,\n 'test_accuracy': 0.9113333333333333,\n 'test_f1': 0.9103169251517195,\n 'test_precision': 0.9060402684563759,\n 'test_recall': 0.9146341463414634,\n 'test_runtime': 27.4206,\n 'test_samples_per_second': 54.703,\n 'test_steps_per_second': 3.428,\n 'epoch': 2.0}"},"metadata":{}}]},{"cell_type":"markdown","source":"Try a BertForSequenceClassification model","metadata":{"id":"iZHJCpckbheJ"}},{"cell_type":"code","source":"model_for_classification = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\")","metadata":{"id":"aQIuQWYaYzu9","outputId":"21df0870-7d4d-4d84-abff-c8ac355e308a","execution":{"iopub.status.busy":"2022-03-26T17:56:28.189143Z","iopub.execute_input":"2022-03-26T17:56:28.191227Z","iopub.status.idle":"2022-03-26T17:56:31.624052Z","shell.execute_reply.started":"2022-03-26T17:56:28.191187Z","shell.execute_reply":"2022-03-26T17:56:31.623350Z"},"trusted":true},"execution_count":25,"outputs":[{"name":"stderr","text":"loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\nModel config BertConfig {\n  \"architectures\": [\n    \"BertForMaskedLM\"\n  ],\n  \"attention_probs_dropout_prob\": 0.1,\n  \"classifier_dropout\": null,\n  \"gradient_checkpointing\": false,\n  \"hidden_act\": \"gelu\",\n  \"hidden_dropout_prob\": 0.1,\n  \"hidden_size\": 768,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 3072,\n  \"layer_norm_eps\": 1e-12,\n  \"max_position_embeddings\": 512,\n  \"model_type\": \"bert\",\n  \"num_attention_heads\": 12,\n  \"num_hidden_layers\": 12,\n  \"pad_token_id\": 0,\n  \"position_embedding_type\": \"absolute\",\n  \"transformers_version\": \"4.16.2\",\n  \"type_vocab_size\": 2,\n  \"use_cache\": true,\n  \"vocab_size\": 30522\n}\n\nloading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f\nSome weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight']\n- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}]},{"cell_type":"code","source":"trainer = Trainer(\n    model=model_for_classification,                         \n    args=training_args,                  \n    train_dataset=train_dataset,         \n    eval_dataset=val_dataset,            \n    compute_metrics = compute_metrics    \n)\n\ntrainer.train()","metadata":{"id":"9UXVmgyMbJ40","outputId":"4cb3b386-57ca-4fe9-dc7d-0306ecd7a1c6","execution":{"iopub.status.busy":"2022-03-26T17:56:31.625358Z","iopub.execute_input":"2022-03-26T17:56:31.625689Z","iopub.status.idle":"2022-03-26T18:04:23.025825Z","shell.execute_reply.started":"2022-03-26T17:56:31.625649Z","shell.execute_reply":"2022-03-26T18:04:23.024971Z"},"trusted":true},"execution_count":26,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  FutureWarning,\n***** Running training *****\n  Num examples = 4000\n  Num Epochs = 2\n  Instantaneous batch size per device = 8\n  Total train batch size (w. parallel, distributed & accumulation) = 8\n  Gradient Accumulation steps = 1\n  Total optimization steps = 1000\nAutomatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='1000' max='1000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1000/1000 07:49, Epoch 2/2]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>10</td>\n      <td>0.750600</td>\n    </tr>\n    <tr>\n      <td>20</td>\n      <td>0.704300</td>\n    </tr>\n    <tr>\n      <td>30</td>\n      <td>0.705200</td>\n    </tr>\n    <tr>\n      <td>40</td>\n      <td>0.709200</td>\n    </tr>\n    <tr>\n      <td>50</td>\n      <td>0.668800</td>\n    </tr>\n    <tr>\n      <td>60</td>\n      <td>0.671700</td>\n    </tr>\n    <tr>\n      <td>70</td>\n      <td>0.691500</td>\n    </tr>\n    <tr>\n      <td>80</td>\n      <td>0.653100</td>\n    </tr>\n    <tr>\n      <td>90</td>\n      <td>0.637700</td>\n    </tr>\n    <tr>\n      <td>100</td>\n      <td>0.621100</td>\n    </tr>\n    <tr>\n      <td>110</td>\n      <td>0.640900</td>\n    </tr>\n    <tr>\n      <td>120</td>\n      <td>0.601400</td>\n    </tr>\n    <tr>\n      <td>130</td>\n      <td>0.601900</td>\n    </tr>\n    <tr>\n      <td>140</td>\n      <td>0.510900</td>\n    </tr>\n    <tr>\n      <td>150</td>\n      <td>0.424200</td>\n    </tr>\n    <tr>\n      <td>160</td>\n      <td>0.240600</td>\n    </tr>\n    <tr>\n      <td>170</td>\n      <td>0.376300</td>\n    </tr>\n    <tr>\n      <td>180</td>\n      <td>0.517000</td>\n    </tr>\n    <tr>\n      <td>190</td>\n      <td>0.455600</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>0.415300</td>\n    </tr>\n    <tr>\n      <td>210</td>\n      <td>0.394300</td>\n    </tr>\n    <tr>\n      <td>220</td>\n      <td>0.357900</td>\n    </tr>\n    <tr>\n      <td>230</td>\n      <td>0.499600</td>\n    </tr>\n    <tr>\n      <td>240</td>\n      <td>0.327100</td>\n    </tr>\n    <tr>\n      <td>250</td>\n      <td>0.370300</td>\n    </tr>\n    <tr>\n      <td>260</td>\n      <td>0.428000</td>\n    </tr>\n    <tr>\n      <td>270</td>\n      <td>0.321500</td>\n    </tr>\n    <tr>\n      <td>280</td>\n      <td>0.200300</td>\n    </tr>\n    <tr>\n      <td>290</td>\n      <td>0.460800</td>\n    </tr>\n    <tr>\n      <td>300</td>\n      <td>0.361800</td>\n    </tr>\n    <tr>\n      <td>310</td>\n      <td>0.288400</td>\n    </tr>\n    <tr>\n      <td>320</td>\n      <td>0.175600</td>\n    </tr>\n    <tr>\n      <td>330</td>\n      <td>0.851300</td>\n    </tr>\n    <tr>\n      <td>340</td>\n      <td>0.364500</td>\n    </tr>\n    <tr>\n      <td>350</td>\n      <td>0.352900</td>\n    </tr>\n    <tr>\n      <td>360</td>\n      <td>0.371800</td>\n    </tr>\n    <tr>\n      <td>370</td>\n      <td>0.370500</td>\n    </tr>\n    <tr>\n      <td>380</td>\n      <td>0.283300</td>\n    </tr>\n    <tr>\n      <td>390</td>\n      <td>0.403000</td>\n    </tr>\n    <tr>\n      <td>400</td>\n      <td>0.357600</td>\n    </tr>\n    <tr>\n      <td>410</td>\n      <td>0.244100</td>\n    </tr>\n    <tr>\n      <td>420</td>\n      <td>0.833000</td>\n    </tr>\n    <tr>\n      <td>430</td>\n      <td>0.395100</td>\n    </tr>\n    <tr>\n      <td>440</td>\n      <td>0.535000</td>\n    </tr>\n    <tr>\n      <td>450</td>\n      <td>0.414800</td>\n    </tr>\n    <tr>\n      <td>460</td>\n      <td>0.372000</td>\n    </tr>\n    <tr>\n      <td>470</td>\n      <td>0.331200</td>\n    </tr>\n    <tr>\n      <td>480</td>\n      <td>0.429600</td>\n    </tr>\n    <tr>\n      <td>490</td>\n      <td>0.983200</td>\n    </tr>\n    <tr>\n      <td>500</td>\n      <td>0.401400</td>\n    </tr>\n    <tr>\n      <td>510</td>\n      <td>0.314300</td>\n    </tr>\n    <tr>\n      <td>520</td>\n      <td>0.195300</td>\n    </tr>\n    <tr>\n      <td>530</td>\n      <td>0.288400</td>\n    </tr>\n    <tr>\n      <td>540</td>\n      <td>0.437800</td>\n    </tr>\n    <tr>\n      <td>550</td>\n      <td>0.332500</td>\n    </tr>\n    <tr>\n      <td>560</td>\n      <td>0.232200</td>\n    </tr>\n    <tr>\n      <td>570</td>\n      <td>0.482800</td>\n    </tr>\n    <tr>\n      <td>580</td>\n      <td>0.268900</td>\n    </tr>\n    <tr>\n      <td>590</td>\n      <td>0.144400</td>\n    </tr>\n    <tr>\n      <td>600</td>\n      <td>0.310400</td>\n    </tr>\n    <tr>\n      <td>610</td>\n      <td>0.271100</td>\n    </tr>\n    <tr>\n      <td>620</td>\n      <td>0.347800</td>\n    </tr>\n    <tr>\n      <td>630</td>\n      <td>0.224500</td>\n    </tr>\n    <tr>\n      <td>640</td>\n      <td>0.318600</td>\n    </tr>\n    <tr>\n      <td>650</td>\n      <td>0.416900</td>\n    </tr>\n    <tr>\n      <td>660</td>\n      <td>0.182700</td>\n    </tr>\n    <tr>\n      <td>670</td>\n      <td>0.169200</td>\n    </tr>\n    <tr>\n      <td>680</td>\n      <td>0.255300</td>\n    </tr>\n    <tr>\n      <td>690</td>\n      <td>0.390500</td>\n    </tr>\n    <tr>\n      <td>700</td>\n      <td>0.100500</td>\n    </tr>\n    <tr>\n      <td>710</td>\n      <td>0.181600</td>\n    </tr>\n    <tr>\n      <td>720</td>\n      <td>0.276300</td>\n    </tr>\n    <tr>\n      <td>730</td>\n      <td>0.332100</td>\n    </tr>\n    <tr>\n      <td>740</td>\n      <td>0.303200</td>\n    </tr>\n    <tr>\n      <td>750</td>\n      <td>0.280700</td>\n    </tr>\n    <tr>\n      <td>760</td>\n      <td>0.095100</td>\n    </tr>\n    <tr>\n      <td>770</td>\n      <td>0.191100</td>\n    </tr>\n    <tr>\n      <td>780</td>\n      <td>0.835100</td>\n    </tr>\n    <tr>\n      <td>790</td>\n      <td>0.087200</td>\n    </tr>\n    <tr>\n      <td>800</td>\n      <td>0.503100</td>\n    </tr>\n    <tr>\n      <td>810</td>\n      <td>0.129500</td>\n    </tr>\n    <tr>\n      <td>820</td>\n      <td>0.269300</td>\n    </tr>\n    <tr>\n      <td>830</td>\n      <td>0.161900</td>\n    </tr>\n    <tr>\n      <td>840</td>\n      <td>0.378300</td>\n    </tr>\n    <tr>\n      <td>850</td>\n      <td>0.200000</td>\n    </tr>\n    <tr>\n      <td>860</td>\n      <td>0.038800</td>\n    </tr>\n    <tr>\n      <td>870</td>\n      <td>0.250200</td>\n    </tr>\n    <tr>\n      <td>880</td>\n      <td>0.301100</td>\n    </tr>\n    <tr>\n      <td>890</td>\n      <td>0.298300</td>\n    </tr>\n    <tr>\n      <td>900</td>\n      <td>0.187500</td>\n    </tr>\n    <tr>\n      <td>910</td>\n      <td>0.185600</td>\n    </tr>\n    <tr>\n      <td>920</td>\n      <td>0.143700</td>\n    </tr>\n    <tr>\n      <td>930</td>\n      <td>0.457400</td>\n    </tr>\n    <tr>\n      <td>940</td>\n      <td>0.257600</td>\n    </tr>\n    <tr>\n      <td>950</td>\n      <td>0.267700</td>\n    </tr>\n    <tr>\n      <td>960</td>\n      <td>0.094800</td>\n    </tr>\n    <tr>\n      <td>970</td>\n      <td>0.144900</td>\n    </tr>\n    <tr>\n      <td>980</td>\n      <td>0.384000</td>\n    </tr>\n    <tr>\n      <td>990</td>\n      <td>0.186700</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>0.067500</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"Saving model checkpoint to ./results/checkpoint-500\nConfiguration saved in ./results/checkpoint-500/config.json\nModel weights saved in ./results/checkpoint-500/pytorch_model.bin\nSaving model checkpoint to ./results/checkpoint-1000\nConfiguration saved in ./results/checkpoint-1000/config.json\nModel weights saved in ./results/checkpoint-1000/pytorch_model.bin\n\n\nTraining completed. Do not forget to share your model on huggingface.co/models =)\n\n\n","output_type":"stream"},{"execution_count":26,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=1000, training_loss=0.3725172662436962, metrics={'train_runtime': 470.3155, 'train_samples_per_second': 17.01, 'train_steps_per_second': 2.126, 'total_flos': 2104888442880000.0, 'train_loss': 0.3725172662436962, 'epoch': 2.0})"},"metadata":{}}]},{"cell_type":"code","source":"trainer.evaluate(eval_dataset=test_dataset, metric_key_prefix=\"test\")","metadata":{"id":"gc3zaGXZf7ZD","execution":{"iopub.status.busy":"2022-03-26T18:04:23.027364Z","iopub.execute_input":"2022-03-26T18:04:23.027804Z","iopub.status.idle":"2022-03-26T18:04:50.439215Z","shell.execute_reply.started":"2022-03-26T18:04:23.027765Z","shell.execute_reply":"2022-03-26T18:04:50.438527Z"},"trusted":true},"execution_count":27,"outputs":[{"name":"stderr","text":"***** Running Evaluation *****\n  Num examples = 1500\n  Batch size = 16\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='94' max='94' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [94/94 00:27]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"EvalPrediction(predictions=array([[-3.0266879 ,  2.781048  ],\n       [-0.30217725,  0.93050414],\n       [ 2.9413102 , -1.2891498 ],\n       ...,\n       [-2.7786074 ,  2.7022812 ],\n       [ 2.4949749 , -0.6932958 ],\n       [-1.2734935 ,  1.7792877 ]], dtype=float32), label_ids=array([1, 1, 0, ..., 1, 0, 1]))\n","output_type":"stream"},{"execution_count":27,"output_type":"execute_result","data":{"text/plain":"{'test_loss': 0.32249999046325684,\n 'test_accuracy': 0.9113333333333333,\n 'test_f1': 0.9105581708137189,\n 'test_precision': 0.9038718291054739,\n 'test_recall': 0.9173441734417345,\n 'test_runtime': 27.402,\n 'test_samples_per_second': 54.741,\n 'test_steps_per_second': 3.43,\n 'epoch': 2.0}"},"metadata":{}}]},{"cell_type":"markdown","source":"Take three sample reviews from IMDB and get predictions for them on the first model","metadata":{"id":"bUrB4_ILdD_T"}},{"cell_type":"code","source":"#very positive Stranger Things review\nst_review = '''\nSuperb series. I am generally not into science fiction, fantasy, supernatural or horror movies or TV series but this is different. Had me hooked from the start and never let go. \nIncredibly intriguing - the mystery surrounding Will, the girl's background and powers, how all these hang together and what forces are at work. \nVery well thought-out plot, superbly executed.\n\nVery engaging too - many likeable characters, all given decent depth. The relationships between all the main characters and how these evolve make the series. \nIf it wasn't for these it would be just another horror series.\n\nMany of the concepts aren't overly new - I was reminded of The Cabin In The Woods, Stand By Me and Cloverfield - but the way everything is brought together is.\n\nGood performances. David Harbour is excellent as Sherriff Hopper and the kids do incredibly well - good casting. \nWinona Ryder, the only big name in the cast (until Sean Astin and Paul Reiser appear in Season 2) is a bit irritating as Joyce Byers, \nthough that might be the fault of her character (and thus the writers and directors). The hysteria is laid on a bit thick...\n\nAfter an excellent Season 1, Seasons 2 and 3 are just as good. There's always a worry with any TV series that the writers run out of \nideas but don't stop production, as the money is too good. So far it is still going strong, and the writers appear keen for it to end on a high.\n'''","metadata":{"id":"---J_788b50Y","execution":{"iopub.status.busy":"2022-03-26T18:04:50.442921Z","iopub.execute_input":"2022-03-26T18:04:50.443502Z","iopub.status.idle":"2022-03-26T18:04:50.448611Z","shell.execute_reply.started":"2022-03-26T18:04:50.443462Z","shell.execute_reply":"2022-03-26T18:04:50.447744Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"#very negative Goat Story review\ngs_review = '''\nIt is an awful movie to watch, its feels so uncomfortable and its impossible to even imagine watching this with the Kids.\n\nIts not an animation for the Kids and it has lots of deviated ideas like a goat's love for its owner moreover goat wants to marry him. \nCutting sbs eyes is violent its so irresponsible to put the title\"child animation\"or \"animation for children\". It should be banned from everywhere.\n'''","metadata":{"id":"dSV6S8rmdlQl","execution":{"iopub.status.busy":"2022-03-26T18:04:50.450059Z","iopub.execute_input":"2022-03-26T18:04:50.450338Z","iopub.status.idle":"2022-03-26T18:04:50.458696Z","shell.execute_reply.started":"2022-03-26T18:04:50.450288Z","shell.execute_reply":"2022-03-26T18:04:50.457923Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"#ambigious Euphoria review (rated 8 stars, should be positive)\neuph_review = '''\nSex drugs and more in high school. First a warning. Although this is a show about high school teenagers it's too much for teens to watch. \nMuch more so than \"13 reasons Why\". There are parts when the scenes are much more explicit than necessary for the story. There isn't a boundary this show doesn't try to push for TV. \nHaving said all that there are some fascinating and bizarre characters that keep things morbidly watchable for 8 episodes. \nA biracial lesbian drug addict, a transgender girl who is her bestie, a jock with demons, his closeted statutory rapist dad, the list goes on.\nIt's hard to keep track of the different characters and plot twists some tunes. Even if you don't agree with the suitability of the content here the acting is better than it has to be. \nZendaya is convincing as an addict very natural acting. Jacob Elordi does the mean jock well.\n\nHate to say it but do want to see what happens in season 2.\n'''","metadata":{"id":"si3TuXTheKKj","execution":{"iopub.status.busy":"2022-03-26T18:04:50.459956Z","iopub.execute_input":"2022-03-26T18:04:50.460838Z","iopub.status.idle":"2022-03-26T18:04:50.471194Z","shell.execute_reply.started":"2022-03-26T18:04:50.460799Z","shell.execute_reply":"2022-03-26T18:04:50.470425Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"def get_prediction(review_text, labels):\n    with torch.no_grad():\n      encoded_review = tokenizer.encode_plus(\n        review_text,\n        max_length=512,\n        add_special_tokens=True,\n        return_token_type_ids=False,\n        padding='max_length',\n        return_attention_mask=True,\n        return_tensors='pt',\n        truncation=True\n      )\n\n      input_ids = encoded_review['input_ids'].to(device)\n      attention_mask = encoded_review['attention_mask'].to(device)\n\n      output = model_agg_cls(input_ids, attention_mask, None, labels.to(device))\n      prediction = torch.argmax(output[1], dim=1)\n    \n      print('predictied label: ', prediction.item())","metadata":{"id":"c1Rrde3Ye5Cz","execution":{"iopub.status.busy":"2022-03-26T18:04:50.472571Z","iopub.execute_input":"2022-03-26T18:04:50.473481Z","iopub.status.idle":"2022-03-26T18:04:50.481697Z","shell.execute_reply.started":"2022-03-26T18:04:50.473367Z","shell.execute_reply":"2022-03-26T18:04:50.480832Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"get_prediction(st_review, labels=torch.LongTensor([1]))","metadata":{"id":"mKpbNoD0iBtP","execution":{"iopub.status.busy":"2022-03-26T18:04:50.483063Z","iopub.execute_input":"2022-03-26T18:04:50.483866Z","iopub.status.idle":"2022-03-26T18:04:50.528091Z","shell.execute_reply.started":"2022-03-26T18:04:50.483829Z","shell.execute_reply":"2022-03-26T18:04:50.527256Z"},"trusted":true},"execution_count":32,"outputs":[{"name":"stdout","text":"predictied label:  1\n","output_type":"stream"}]},{"cell_type":"code","source":"get_prediction(gs_review, labels=torch.LongTensor([0]))","metadata":{"id":"mWhnUF2YiGu5","execution":{"iopub.status.busy":"2022-03-26T18:04:50.529480Z","iopub.execute_input":"2022-03-26T18:04:50.530301Z","iopub.status.idle":"2022-03-26T18:04:50.561410Z","shell.execute_reply.started":"2022-03-26T18:04:50.530259Z","shell.execute_reply":"2022-03-26T18:04:50.560638Z"},"trusted":true},"execution_count":33,"outputs":[{"name":"stdout","text":"predictied label:  0\n","output_type":"stream"}]},{"cell_type":"code","source":"get_prediction(euph_review, labels=torch.LongTensor([0]))","metadata":{"id":"UwYjW0VCiN0v","execution":{"iopub.status.busy":"2022-03-26T18:04:50.562681Z","iopub.execute_input":"2022-03-26T18:04:50.563437Z","iopub.status.idle":"2022-03-26T18:04:50.597054Z","shell.execute_reply.started":"2022-03-26T18:04:50.563399Z","shell.execute_reply":"2022-03-26T18:04:50.596258Z"},"trusted":true},"execution_count":34,"outputs":[{"name":"stdout","text":"predictied label:  1\n","output_type":"stream"}]},{"cell_type":"markdown","source":"All the predictions are right","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}